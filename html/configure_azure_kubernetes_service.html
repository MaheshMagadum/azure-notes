<a href='../index.html'>Back to topics</a><style> #notes_table {	font-family: arial, sans-serif; border-collapse: collapse;  width: 100%; background-color: #ffffff;	color:black;}#notes_table td, #notes_table th { text-align: left; padding: 8px; border: 1px solid #808080;}#notes_table tr:nth-child(even) {background-color: white;}#notes_table tr:hover {	background-color:  #ffffff;}#notes_table tr > th {	background: #f8f8f8; color: black;}</style><table  id='notes_table'><tr align= left><th>Azure Kubernetes Service(AKS) terminology</th></ul></td></tr><td><ul><li>Azure Kubernetes Service (AKS) provides a simplified approach for deploying a managed Kubernetes cluster in Azure</li><li>AKS employs components like nodes, pods, and pools to help you deploy and manage your container applications in Kubernetes clusters</li><li>{'Pools': 'A pool is a group of nodes that have an identical configuration.'}</li><li>{'Nodes': 'A node is an individual virtual machine that runs containerized applications.'}</li><li>{'Pods': 'A pod is a single instance of an application. A pod can contain multiple containers.'}</li><li>{'Container': 'A container is a lightweight and portable executable image that contains software and all of its dependencies.'}</li><li>{'Deployment': 'A deployment has one or more identical pods managed by Kubernetes.'}</li><li>{'Manifest': 'The manifest is the YAML file that describes a deployment.'}</li><tr align= left><th>Closer look on AKS clusters, nodes, and pools</th></ul></td></tr><td><ul><li>To run your applications and supporting services, you need a Kubernetes node for your AKS cluster</li><li>Each AKS cluster contains one or more nodes that run the Kubernetes node components and the container runtime.</li><li>Nodes are instances of Azure Virtual Machines. Nodes of the same configuration are grouped together into node pools</li><li>A Kubernetes cluster contains one or more node pools.</li><li>The initial number of nodes and size are defined when you create an AKS cluster</li><li>When you create an AKS cluster, an Azure-managed cluster node is automatically created and configured</li><li>The kubelet is the Kubernetes agent that processes the orchestration requests from the Azure-managed node, and scheduling of running the requested containers</li><li>The kube-proxy component handles virtual networking on each node. The proxy routes network traffic and manages IP addressing for services and pods.</li><li>The container runtime component allows containerized applications to run and interact with other resources such as the virtual network and storage.</li><li>AKS clusters with Kubernetes version 1.19 node pools and later use containerd as the container runtime.</li><li>AKS clusters with node pools that use Kubernetes versions earlier than v1.19 implement Moby (upstream Docker) as the container runtime.</li><li>When you implement Azure Kubernetes Service clusters, you pay only for running agent nodes in your cluster</li><tr align= left><th>Azure Kubernetes Service networking</th></ul></td></tr><td><ul><li>Kubernetes provides an abstraction layer for virtual networking to allow access to your applications, or for application components to communicate with each other</li><tr align= left><th>Things to know on Kubernetes virtual networking</th></ul></td></tr><td><ul><li>Kubernetes nodes are connected to a virtual network, which provides inbound and outbound connectivity for pods.</li><li>The kube-proxy component runs on each node to provide the network features.</li><li>Network policies configure security and filtering of the network traffic for pods.</li><li>Network traffic can be distributed by using a load balancer.</li><li>Complex routing of application traffic can be achieved with Ingress Controllers.</li><tr align= left><th>Azure Kubernetes Service</th></ul></td></tr><td><ul><li>The Azure platform helps to simplify virtual networking for Azure Kubernetes Service clusters.</li><tr align= left><th>Things to know about Kubernetes service types</th></ul></td></tr><td><ul><li>Cluster IP (internal only) - Create an internal IP address for use within an Azure Kubernetes Service cluster.</li><li>NodePort - Create a port mapping on the underlying node.</li><li>LoadBalancer - Create an Azure Load Balancer resource, configure an external IP address, and connect the requested pods to the load balancer back-end pool.</li><li>ExternalName - Create a specific DNS entry.</li><tr align= left><th>some details about these network configuration options</th></ul></td></tr><td><ul><li>You can create internal and external load balancers.</li><li>The IP address for load balancers and services can be dynamically assigned, or you can specify an existing static IP address.</li><li>Internal load balancers are only assigned a private IP address, so can't be accessed from the internet.</li><li>Both internal and external static IP addresses can be assigned. The existing static IP address is often tied to a DNS entry.</li><tr align= left><th>Things to know about Kubernetes pods</th></ul></td></tr><td><ul><li>Pods typically have a 1:1 mapping with a container, although there are advanced scenarios where a pod might contain multiple containers.</li><li>Multi-container pods are scheduled together on the same node, and allow containers to share related resources.</li><li>When you create a pod, you can define resource limits to request a certain amount of CPU or memory resources</li><li>You can specify maximum resource limits that prevent a given pod from consuming too much compute resource from the underlying node.</li><li>{'Note': 'A best practice is to include resource limits for all pods to help the Kubernetes Scheduler recognize what resources are needed and permitted.'}</li><li>A pod is a logical resource, but a container is where the application workloads run.</li><li>Pods are typically ephemeral, disposable resources.</li><li>pods are commonly deployed and managed by Kubernetes controllers, such as the Deployment controller.</li><tr align= left><th>Azure Kubernetes Service storage</th></ul></td></tr><td><ul><li>There are different scenarios where applications in an Azure Kubernetes Service cluster might need to store and retrieve data.</li><tr align= left><th>know about storage volumes</th></ul></td></tr><td><ul><li>You can use Azure Disks or Azure Files to provide a persistent volume</li><li>A persistent volume can be statically created by a cluster administrator, or dynamically created by the Kubernetes API server.</li><li>If a pod is scheduled, and requests Storage that's not currently available, Kubernetes can create the underlying Azure Disks or Azure Files storage.</li><li>Dynamic provisioning uses a StorageClass type to identify what kind of Azure Storage needs to be created.</li><tr align= left><th>know about storage classes</th></ul></td></tr><td><ul><li>The StorageClass type also defines the reclaimPolicy actions for the storage</li><li>The reclaimPolicy definition controls the behavior of the underlying Azure Storage resource when the pod is deleted and the persistent volume might no longer be required</li><li>The underlying Storage resource can be deleted, or retained for use with a future pod.</li><li>n Azure Kubernetes Service, four initial StorageClasses types are created for a cluster by using in-tree storage plugins -></li><li>{'StorageClass type': 'default, managed-premium, azurefile, azurefile-premium'}</li><li>{'For all above SC types': 'reclaimPolicy action ensures the underlying Azure disk is deleted when the persistent volume that used the disk is deleted.'}</li><li>If no StorageClass type is specified for a persistent volume, the default type is used.</li><tr align= left><th>Know about persistent volume claims</th></ul></td></tr><td><ul><li>A persistent volume claim (PersistentVolumeClaim) requests either Azure Disks or Azure Files storage of a particular StorageClass, access mode, and size</li><li>The Kubernetes API server can dynamically provision the underlying storage resource in Azure, if there's no existing resource to fulfill the claim based on the defined StorageClass type.</li><li>The pod definition includes the volume mount after the volume has been connected to the pod.</li><li>A persistent volume is bound to a persistent volume claim after an available Storage resource is assigned to the pod that requests the volume.</li><li>There's a 1:1 mapping of persistent volumes to claims.</li><tr align= left><th>Azure Kubernetes Service scaling</th></ul></td></tr><td><ul><li>Scaling techniques below</li><li>Manually scale pods or nodes</li><li>Automatically scale pods. Use the horizontal pod autoscaler (HPA) to monitor resource demand and automatically scale the number of your replicas.</li><li>Automatically scale clusters. Respond to changing pod demands with the cluster autoscaler, which adjusts the number of your nodes based on the requested compute resources in the node pool</li><li>By default, the cluster autoscaler checks the API server every 10 seconds for any required changes in the node count. If the cluster autoscale determines a change is required, the number of nodes in your AKS cluster is increased or decreased accordingly.</li><tr align= left><th>Things to consider when using horizontal autoscaling</th></ul></td></tr><td><ul><li>Consider number of pods (replicas). When you configure the HPA for a given deployment, you define the minimum and maximum number of pods (replicas) that can run.</li><li>Consider scaling metrics. To use the HPA, define the metric to monitor and to use as the basis for scaling decisions, such as CPU usage.</li><li>Consider cooldown for scaling events. As the HPA checks the Metrics API every 30 seconds, previous scale events might not complete before subsequent checks occur. The HPA might change the number of replicas before the previous scale event receives the application workload and resource demands to adjust accordingly.</li><li>So, To minimize race events, set cooldown or delay values to define how long the HPA must wait after a scale event before another scale event is triggered</li><li>Consider tuning cooldown values. You might need to tune cooldown values. Default cooldown values might give the impression that the HPA isn't scaling the replica count quickly enough</li><li>To more quickly increase the number of replicas in use, reduce the --horizontal-pod-autoscaler-upscale-delay value when you create your HPA definitions by using the Azure CLI kubectl tool.</li><tr align= left><th>Things to consider when using cluster autoscaling</th></ul></td></tr><td><ul><li>Consider combining with HPA. Cluster autoscaler is typically used alongside the horizontal pod autoscaler.</li><li>When the two scaling techniques are combined, the HPA increases or decreases the number of pods based on application demand</li><li>The cluster autoscaler adjusts the number of nodes as needed to run the extra pods accordingly.</li><li>Consider scale-out events. If a node doesn't have sufficient compute resources to run a requested pod, that pod can't progress through the scheduling process</li><li>The pod can't start unless other compute resources are available within the node pool.</li><li>When the cluster autoscaler notices pods that can't be scheduled due to node pool resource constraints, the number of nodes within the node pool is increased to provide the extra compute resources</li><li>Consider burst scaling to Azure Container Instances.  If your application needs to scale rapidly, some pods might remain in a state waiting to be scheduled until the new nodes deployed by the cluster autoscaler can accept the scheduled pods</li><li>For applications that have high burst demands, you can scale with virtual nodes and Azure Container Instances</li><li>Consider scale-in events. The cluster autoscaler monitors the pod scheduling status for nodes that haven't recently received new scheduling requests</li><li>This scenario indicates that the node pool has more compute resources than required, so the number of nodes can be decreased.</li><li>A node that passes a threshold for not being needed for 10 minutes is scheduled for deletion by default.</li><li>When this situation occurs, pods are scheduled to run on other nodes within the node pool, and the cluster autoscaler decreases the number of nodes.</li><li>Consider avoiding single pods. Your applications might experience some disruption as pods are scheduled on different nodes when the cluster autoscaler decreases the number of nodes</li><li>To minimize disruption, avoid applications that use a single pod instance.</li><tr align= left><th>Configure AKS burst scaling to Azure Container Instances</th></ul></td></tr><td><ul><li>To resolve this situation, you can rapidly scale your Azure Kubernetes Service cluster by integrating with Azure Container Instances</li><li>Understand what is the situation here! HPA migh schedule more pods that can be provided by the existing compute resources in the node pool (but limitted nodes?)</li><tr align= left><th>Things to know about rapid burst scaling</th></ul></td></tr><td><ul><li>Azure Container Instances lets you quickly deploy your container instance without extra infrastructure overhead</li><li>The Virtual Kubelet component is installed in your AKS cluster. The component presents your container instance as a virtual Kubernetes node.</li><li>Kubernetes schedules pods to run as container instances through virtual nodes, rather than pods on virtual machine nodes directly in your AKS cluster.</li><li>Virtual Node? Interesting!</li><li>Your application requires no modification to use virtual nodes.</li><li>Deployments can scale across AKS and Container Instances. There's no delay when the cluster autoscaler deploys new nodes in your AKS cluster.</li><li>Virtual nodes are deployed to another subnet in the same virtual network as your AKS cluster.</li><li>This virtual network configuration allows the traffic between Container Instances and AKS to be secured</li><li>Like an AKS cluster, a container instance is a secure, logical compute resource that's isolated from other users.</li><tr align= left><th>More</th></ul></td></tr><td><ul><li>h</li><li>t</li><li>t</li><li>p</li><li>s</li><li>:</li><li>/</li><li>/</li><li>l</li><li>e</li><li>a</li><li>r</li><li>n</li><li>.</li><li>m</li><li>i</li><li>c</li><li>r</li><li>o</li><li>s</li><li>o</li><li>f</li><li>t</li><li>.</li><li>c</li><li>o</li><li>m</li><li>/</li><li>e</li><li>n</li><li>-</li><li>u</li><li>s</li><li>/</li><li>t</li><li>r</li><li>a</li><li>i</li><li>n</li><li>i</li><li>n</li><li>g</li><li>/</li><li>m</li><li>o</li><li>d</li><li>u</li><li>l</li><li>e</li><li>s</li><li>/</li><li>i</li><li>n</li><li>t</li><li>r</li><li>o</li><li>-</li><li>t</li><li>o</li><li>-</li><li>k</li><li>u</li><li>b</li><li>e</li><li>r</li><li>n</li><li>e</li><li>t</li><li>e</li><li>s</li><li>/</li></table><a href='../index.html'>Back to topics</a>